{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import csv \n",
    "from parsel import Selector\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Class to describe a given company \n",
    "    @field: name: a string for the company name\n",
    "    @field: description: a string for the company description\n",
    "    @field: founders: a list of Founder objects\n",
    "    @field: industries: a list of strings for different industries\n",
    "    @field: website: a string for the website\n",
    "    @field: lastStage: a string for the last stage of funding (eg. Series A)\n",
    "    @field: linkedin: a string for the company's LinkedIn profile\n",
    "    @field: location: a string for the company's location\n",
    "'''\n",
    "class Company:\n",
    "    def __init__(self, companyName):\n",
    "        self.name = companyName\n",
    "        self.description = None\n",
    "        self.founders = []\n",
    "        self.industries = []\n",
    "        self.website = None\n",
    "        self.lastStage = None\n",
    "        self.linkedin = None\n",
    "    \n",
    "    def toJson(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__)\n",
    "    \n",
    "'''\n",
    "Class to describe a founder\n",
    "    @field: name: a string for the founder's name\n",
    "    @field: education: an list of education objects\n",
    "    @field: experience: a list of experience objects\n",
    "'''\n",
    "class Founder:\n",
    "    def __init__(self, founderName):\n",
    "        self.name = founderName\n",
    "        self.education = []\n",
    "        self.experience = []\n",
    "\n",
    "'''\n",
    "Class to help describe a founder's education\n",
    "    @field: degree: a string to describe the degree objective\n",
    "    @field: school: a string for the school attended\n",
    "    @field: field: a string to describe the major\n",
    "'''\n",
    "class Education:\n",
    "    def __init__(self, schoolName):\n",
    "        self.school = schoolName\n",
    "        self.degree = None\n",
    "        self.field = None\n",
    "'''\n",
    "Class to help describe a founder's experience\n",
    "    @field: companyName: a string to describe the company's name\n",
    "    @field: title: a string to describe the title held\n",
    "    @field: description: a string to describe the job description\n",
    "'''       \n",
    "class Experience:\n",
    "    def __init__(self, companyName):\n",
    "        self.companyName = companyName\n",
    "        self.title = None\n",
    "        self.dates = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mymacbook/.local/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Loads the dataframe from Query 1 and Query 2 and merges + drops duplicates and NaNs\n",
    "    @param: csv1: path to CSV 1 (formed by Query 1)\n",
    "    @param: csv2: path to CSV 2 (formed by Query 2)\n",
    "    @return: df1: a merged dataframe of csv1 and csv2\n",
    "'''\n",
    "def loadBacktestData(csv1, csv2):\n",
    "    df1 = pd.read_csv(csv1)\n",
    "    df2 = pd.read_csv(csv2)\n",
    "    df1 = df1.append(df2)\n",
    "    df1 = df1.drop_duplicates(subset=['Organization Name'])\n",
    "    df1 = df1[df1.Founders.notna()]\n",
    "    df1 = df1[df1.LinkedIn.notna()]\n",
    "    df1 = df1.reset_index()\n",
    "    for i in range(len(df1)):\n",
    "        if df1.iloc[i].LinkedIn.count('about') > 0:\n",
    "            df1['LinkedIn'][i] = df1['LinkedIn'][i].split('about')[0]\n",
    "    return df1\n",
    "\n",
    "# Loads and combines both CSVs into df1 dataframe\n",
    "df1 = loadBacktestData('backtest1.csv', 'backtest2.csv')\n",
    "# Creates a company_data dictionary to store scraped data\n",
    "company_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Method to set up and log into the LinkedIn using chromedriver\n",
    "    @param: driverPath: path to the chromedriver.exe file\n",
    "    @param: liUsername: string of LI username\n",
    "    @param: liPassword: string of LI password\n",
    "    @return: driver: the Chrome Webdriver (can be passed into future function arguments)\n",
    "'''\n",
    "def setupDriver(driverPath, liUsername, liPassword):\n",
    "    # Sets up Chrome Webdriver and navigates to LinkedIn\n",
    "    driver = webdriver.Chrome(driverPath)\n",
    "    driver.get('https://www.linkedin.com/')\n",
    "    sleep(2.0)\n",
    "    \n",
    "    # Signs in with given credentials and returns the driver\n",
    "    driver.find_element_by_xpath('//a[text()=\"Sign in\"]').click()\n",
    "    sleep(2.0)\n",
    "    username_input = driver.find_element_by_name('session_key')\n",
    "    username_input.send_keys(liUsername)\n",
    "    password_input = driver.find_element_by_name('session_password')\n",
    "    password_input.send_keys(liPassword)\n",
    "    sleep(2.0)\n",
    "    driver.find_element_by_xpath('//button[text()=\"Sign in\"]').click()\n",
    "    return driver\n",
    "\n",
    "# Launches LinkedIn and logs in\n",
    "driver = setupDriver('./chromedriver', 'LIUsername', 'LIPassword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Method to add LI information for a company into the company_data dictionary with a new company object\n",
    "    @param: entry: a pandas series extracted from a single row in the dataframe from loadBacktestData\n",
    "    @return: None\n",
    "This method updates the company_data dictionary and returns nothing\n",
    "'''\n",
    "def extractLIInfo(entry):\n",
    "    # Adds a new company entry to the company_data dictionary and populates fields\n",
    "    company_ = Company(entry['Organization Name'])\n",
    "    company_.description = entry['Description']\n",
    "    company_.industries = [i.strip() for i in entry['Industries'].split(',')]\n",
    "    company_.website = entry['Website']\n",
    "    company_.lastStage = entry['Last Funding Type']\n",
    "    # Edge case where the LinkedIn link does not end in '/'\n",
    "    if entry['LinkedIn'][-1] != '/':\n",
    "        entry['LinkedIn'] = entry['LinkedIn'] + '/'\n",
    "    company_.linkedin = entry['LinkedIn']\n",
    "    company_.location = entry['Headquarters Location']\n",
    "    \n",
    "    # Generates a list of founder names\n",
    "    founderNames = [i.strip() for i in entry['Founders'].split(',')]\n",
    "    # Navigates to the company's primary LinkedIn page [People Tab]\n",
    "    try:\n",
    "        driver.get(entry['LinkedIn'] + \"people/\")\n",
    "        sleep(1.0)\n",
    "        # For each founder in the list, the school name, degree, and major is extracted\n",
    "        for founder in founderNames:\n",
    "            name_input = driver.find_element_by_id('people-search-keywords')\n",
    "            name_input.send_keys(founder)\n",
    "            driver.find_element_by_id(\"people-search-keywords\").send_keys(Keys.ENTER)\n",
    "            sleep(1.0)\n",
    "            try:\n",
    "                driver.find_element_by_xpath('//a[@data-control-name = \"people_profile_card_name_link\"]').click()\n",
    "                sleep(2.0)\n",
    "                # Scrolls to the bottom of the webpage (if no scroll, error where the full webpage doens't load)\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/2);\")\n",
    "                sleep(0.75)\n",
    "                # Creates a founder object for the given founder\n",
    "                founder_ = Founder(founder)\n",
    "\n",
    "                ###########################################################\n",
    "                # Extracts degree information (formatted as a list of items)\n",
    "                schools = driver.find_elements_by_xpath('//div[@class=\"pv-entity__degree-info\"]')\n",
    "                for school in schools:\n",
    "                    school_ = school.text.split('\\n')\n",
    "                    # The school is formatted as a list [School Name,'Degree Name',Degree Name,'Field of Study',FOS]\n",
    "                    try:\n",
    "                        educ_ = Education(school_[0])\n",
    "                        try:\n",
    "                            educ_.degree = school_[2]\n",
    "                            educ_.field = school_[4]\n",
    "                        except:\n",
    "                            pass\n",
    "                        # Appends the temporary education object to the founder\n",
    "                        founder_.education.append(educ_)\n",
    "                    except:\n",
    "                        print(\"No schools\")\n",
    "                        pass\n",
    "\n",
    "                ###############################################################    \n",
    "                # Extracts experience information (formatted as a list of items)\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/3);\")\n",
    "                sleep(1.0)\n",
    "                experiences = driver.find_elements_by_xpath('//a[@data-control-name=\"background_details_company\"]')\n",
    "                for exp in experiences:\n",
    "                    exp_lst = exp.text.split('\\n')\n",
    "                    # The exp_ is formatted as [title, 'companyname', companyname, 'datesemployed', datesemployed, ..]\n",
    "                    try:\n",
    "                        if exp_lst[0] == 'Company Name':\n",
    "                            exp_ = Experience(exp_lst[1])\n",
    "                            founder_.experience.append(exp_)\n",
    "                        else:\n",
    "                            exp_ = Experience(exp_lst[2])\n",
    "                            try:\n",
    "                                exp_.title = exp_lst[0]\n",
    "                                exp_.dates = exp_lst[4]\n",
    "                                founder_.experience.append(exp_)\n",
    "                            except:\n",
    "                                founder_.experience.append(exp_)\n",
    "                    except:\n",
    "                        print(\"No experience\")\n",
    "                        pass\n",
    "                # Appends the temporary founder object to the company\n",
    "                company_.founders.append(founder_)\n",
    "                # Re-navigates to company's LinkedIn page\n",
    "                driver.get(entry['LinkedIn'] + \"people/\")\n",
    "                sleep(1.0)\n",
    "            except:\n",
    "                founder_ = Founder(founder)\n",
    "                company_.founders.append(founder_)\n",
    "                print(\"{} not found for {}\".format(company_.name, founder_.name))\n",
    "                driver.get(entry['LinkedIn'] + \"people/\")\n",
    "                sleep(1.0)\n",
    "    except:\n",
    "        print(\"LinkedIn page doesn't exist\")\n",
    "    # Adds the company to the company_data dictionary\n",
    "    company_data[company_.name] = company_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Method to save the company data as a txt file (loadable as JSON)\n",
    "    @param: data: company_data dictionary of Company objects\n",
    "    @return: None\n",
    "'''\n",
    "def saveData(data, fileName):\n",
    "    data_json = json.dumps(data, default=lambda x: x.__dict__)\n",
    "    with open(fileName, 'w') as outfile:\n",
    "        json.dump(data_json, outfile)\n",
    "\n",
    "'''\n",
    "Method to load company data into a dictionary (same structure as Company object)\n",
    "    @param: dataFile: string path to saved txt file\n",
    "    @return: dataDict: a dictionary with the same structure as a Company object\n",
    "'''\n",
    "def loadData(dataFile):\n",
    "    with open(dataFile) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    dataDict = json.loads(data)\n",
    "    return dataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "Streamlit not found for Adrien Treuille\n",
      "Streamlit not found for Amanda Kelly\n",
      "Streamlit not found for Thiago Teixeira\n",
      "13\n",
      "Anyscale not found for Ion Stoica\n",
      "Anyscale not found for Michael I. ​ Jordan\n",
      "LinkedIn page doesn't exist\n",
      "14\n",
      "LinkedIn page doesn't exist\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mymacbook/.local/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinkedIn page doesn't exist\n",
      "16\n",
      "LinkedIn page doesn't exist\n",
      "17\n",
      "LinkedIn page doesn't exist\n",
      "18\n",
      "LinkedIn page doesn't exist\n",
      "19\n",
      "LinkedIn page doesn't exist\n",
      "20\n",
      "LinkedIn page doesn't exist\n",
      "21\n",
      "LinkedIn page doesn't exist\n",
      "22\n",
      "LinkedIn page doesn't exist\n",
      "23\n",
      "LinkedIn page doesn't exist\n",
      "24\n",
      "LinkedIn page doesn't exist\n",
      "25\n",
      "LinkedIn page doesn't exist\n",
      "26\n",
      "LinkedIn page doesn't exist\n",
      "27\n",
      "LinkedIn page doesn't exist\n",
      "28\n",
      "LinkedIn page doesn't exist\n",
      "29\n",
      "LinkedIn page doesn't exist\n",
      "30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-11d1466748a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mextractLIInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Stores data for a single company in company_data\n",
    "for i in range(12,100):\n",
    "    print(i)\n",
    "    if i%5 == 0:\n",
    "        sleep(5.0)\n",
    "    extractLIInfo(df1.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveData(company_data, 'data100.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = loadData('data.txt')\n",
    "data2 = loadData('data2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = {**data1, **data2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.linkedin.com/in/amarhanspal/\")\n",
    "sleep(2.0)\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/2);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiences = driver.find_elements_by_xpath('//a[@data-control-name=\"background_details_company\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Company Name', 'Autodesk', 'Total Duration', '15 yrs 6 mos']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiences[3].text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = extractLIInfo(df1.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df1.LinkedIn:\n",
    "    if i[:4] != \"http\":\n",
    "        print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.linkedin.com/company/metropolisio/'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.iloc[20].LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bright Machines': <__main__.Company at 0x7fbd72d1fb90>,\n",
       " 'Nuvia': <__main__.Company at 0x7fbd72d25f10>,\n",
       " 'Vianai': <__main__.Company at 0x7fbd7297d0d0>,\n",
       " 'Kodiak Robotics': <__main__.Company at 0x7fbd72d0bfd0>,\n",
       " 'Contentstack': <__main__.Company at 0x7fbd74930790>,\n",
       " 'Beyond Identity': <__main__.Company at 0x7fbd7297d290>,\n",
       " 'Observe.AI': <__main__.Company at 0x7fbd7069d0d0>,\n",
       " 'Bison Trails': <__main__.Company at 0x7fbd749421d0>,\n",
       " 'Compound': <__main__.Company at 0x7fbd74942090>,\n",
       " 'Kerauno': <__main__.Company at 0x7fbd74944490>,\n",
       " 'PlanetScale': <__main__.Company at 0x7fbd73d20890>,\n",
       " 'Lightmatter': <__main__.Company at 0x7fbd749442d0>,\n",
       " 'Streamlit': <__main__.Company at 0x7fbd72d239d0>,\n",
       " 'Anyscale': <__main__.Company at 0x7fbd72ea4d90>,\n",
       " 'Nightfall AI': <__main__.Company at 0x7fbd74944290>,\n",
       " 'Tecton': <__main__.Company at 0x7fbd72e9f9d0>,\n",
       " 'Oxide Computer Company': <__main__.Company at 0x7fbd72e97b50>,\n",
       " 'DataGuard': <__main__.Company at 0x7fbd73347f10>,\n",
       " 'Docly International AB': <__main__.Company at 0x7fbd72e971d0>,\n",
       " 'Persona': <__main__.Company at 0x7fbd7341a550>,\n",
       " 'Metropolis': <__main__.Company at 0x7fbd7341a410>,\n",
       " 'One': <__main__.Company at 0x7fbd72ecf650>,\n",
       " 'Netdata': <__main__.Company at 0x7fbd731b2bd0>,\n",
       " 'Epsagon': <__main__.Company at 0x7fbd7297ce10>,\n",
       " 'Lively': <__main__.Company at 0x7fbd72d978d0>,\n",
       " 'BRYTER': <__main__.Company at 0x7fbd7338a850>,\n",
       " 'evervault': <__main__.Company at 0x7fbd73d26dd0>,\n",
       " 'Alchemy': <__main__.Company at 0x7fbd706db810>,\n",
       " 'Habu': <__main__.Company at 0x7fbd72d81550>,\n",
       " 'CoachHub': <__main__.Company at 0x7fbd706e7090>,\n",
       " 'Axis Security': <__main__.Company at 0x7fbd706e5710>,\n",
       " 'MetaCX': <__main__.Company at 0x7fbd734e1ad0>,\n",
       " 'Deel': <__main__.Company at 0x7fbd731b2b10>,\n",
       " 'Ethyca': <__main__.Company at 0x7fbd73d26cd0>,\n",
       " 'Sirona Medical': <__main__.Company at 0x7fbd739c9650>,\n",
       " 'ProdPerfect': <__main__.Company at 0x7fbd73999810>,\n",
       " 'GRAX': <__main__.Company at 0x7fbd734c9b10>,\n",
       " 'Crossbeam': <__main__.Company at 0x7fbd73347110>,\n",
       " 'NEAR Protocol': <__main__.Company at 0x7fbd73498a90>,\n",
       " 'Zippin': <__main__.Company at 0x7fbd734e8ad0>,\n",
       " 'GitGuardian': <__main__.Company at 0x7fbd73999990>,\n",
       " 'TWAICE': <__main__.Company at 0x7fbd73994b10>,\n",
       " 'Hypersonix': <__main__.Company at 0x7fbd731b2f50>,\n",
       " 'SignalWire': <__main__.Company at 0x7fbd73d26650>,\n",
       " 'Brightback': <__main__.Company at 0x7fbd7349cbd0>,\n",
       " 'Chronosphere': <__main__.Company at 0x7fbd73178390>,\n",
       " 'Pearl': <__main__.Company at 0x7fbd734dc410>,\n",
       " 'Remote': <__main__.Company at 0x7fbd706e6d50>,\n",
       " 'Solo.io': <__main__.Company at 0x7fbd706df450>,\n",
       " 'Run The World': <__main__.Company at 0x7fbd73497f10>,\n",
       " 'RADAR': <__main__.Company at 0x7fbd734e8610>,\n",
       " 'Stellar Health': <__main__.Company at 0x7fbd706e3c10>,\n",
       " 'AppOmni': <__main__.Company at 0x7fbd731b2350>,\n",
       " 'Span.IO': <__main__.Company at 0x7fbd734ce390>,\n",
       " 'Ordway': <__main__.Company at 0x7fbd72d91b10>,\n",
       " 'Forager': <__main__.Company at 0x7fbd739e2750>,\n",
       " 'Everee': <__main__.Company at 0x7fbd734d3e10>,\n",
       " 'Dura Software': <__main__.Company at 0x7fbd734d3990>,\n",
       " 'Subtree Inc.': <__main__.Company at 0x7fbd734d6fd0>,\n",
       " 'Auterion': <__main__.Company at 0x7fbd74941650>,\n",
       " 'Instagrid': <__main__.Company at 0x7fbd73a5db90>,\n",
       " 'Postal.io': <__main__.Company at 0x7fbd73994bd0>,\n",
       " 'AKUR8': <__main__.Company at 0x7fbd731b2c10>,\n",
       " 'Rivet': <__main__.Company at 0x7fbd734d32d0>,\n",
       " 'Jumbo Privacy': <__main__.Company at 0x7fbd7489c7d0>,\n",
       " 'Kaskada': <__main__.Company at 0x7fbd7489c490>,\n",
       " 'KyckGlobal, Inc.': <__main__.Company at 0x7fbd72d93ad0>,\n",
       " 'Quartz Systems, Inc.': <__main__.Company at 0x7fbd73d26990>,\n",
       " 'Sila': <__main__.Company at 0x7fbd706dbc90>,\n",
       " 'Flatfile': <__main__.Company at 0x7fbd731726d0>,\n",
       " 'Spectro Cloud': <__main__.Company at 0x7fbd706dde50>,\n",
       " 'Truekx': <__main__.Company at 0x7fbd731b2b50>,\n",
       " 'Shujinko': <__main__.Company at 0x7fbd73174210>,\n",
       " 'Opora': <__main__.Company at 0x7fbd734c9f50>,\n",
       " 'Pinwheel': <__main__.Company at 0x7fbd73d23790>,\n",
       " 'Peregrine Technologies': <__main__.Company at 0x7fbd72ea5990>,\n",
       " 'SYNAOS': <__main__.Company at 0x7fbd731748d0>,\n",
       " 'Trainual': <__main__.Company at 0x7fbd72d90890>,\n",
       " 'Majelan': <__main__.Company at 0x7fbd72ef2e10>,\n",
       " 'Freedom Robotics': <__main__.Company at 0x7fbd74904810>,\n",
       " 'Acre': <__main__.Company at 0x7fbd72e9d610>,\n",
       " 'Syncari': <__main__.Company at 0x7fbd74904610>,\n",
       " 'SoundCommerce': <__main__.Company at 0x7fbd72d81810>,\n",
       " 'Strala': <__main__.Company at 0x7fbd7489c350>,\n",
       " 'Acquire': <__main__.Company at 0x7fbd727debd0>,\n",
       " 'Samya.AI': <__main__.Company at 0x7fbd72e97250>,\n",
       " 'Swit': <__main__.Company at 0x7fbd727de210>,\n",
       " 'OneRail': <__main__.Company at 0x7fbd74904310>,\n",
       " 'BuildOps': <__main__.Company at 0x7fbd73d230d0>,\n",
       " 'Trust.co': <__main__.Company at 0x7fbd73d1dc90>,\n",
       " 'Capmo': <__main__.Company at 0x7fbd73a67050>,\n",
       " 'Athena Security': <__main__.Company at 0x7fbd734dc910>,\n",
       " 'BlocWatch': <__main__.Company at 0x7fbd73d23310>,\n",
       " 'SKUxchange': <__main__.Company at 0x7fbd72d88790>,\n",
       " 'Synthetic Minds': <__main__.Company at 0x7fbd72d81e90>,\n",
       " 'api.video': <__main__.Company at 0x7fbd72981e50>,\n",
       " 'Robocorp': <__main__.Company at 0x7fbd7489c8d0>,\n",
       " 'Osano': <__main__.Company at 0x7fbd727de610>,\n",
       " 'Nuvocargo': <__main__.Company at 0x7fbd74904710>,\n",
       " 'Co–Star': <__main__.Company at 0x7fbd73d1db90>}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Miniconda3",
   "language": "python",
   "name": "miniconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
